# Ch2. 프로그램이 실행되었지만, 뭐가 뭔지 하나도 모르겠다

## 프로그램의 런타임에는 무슨 일이 일어날까?

정적 코드 → 기계어로 번역 → 이를 읽고 수행하는 CPU는 2가지만 알고 있음을 전제로 한다.

1. 메모리에서 명령어를 가져온다 (**`dispatch`**)
    
    PC(Program Counter)라는 레지스터에 다음 실행할 명령어 주소를 저장하고 가져오는 방식
    
    **PC 레지스터란? 용량은 매우 작지만 속도가 매우 빠른 일종의 메모리 (주소는 코드의 순차적인 실행순서에 따라 1씩 자동으로 증가하고, 순차를 깨뜨리는 명령이 있다면 동적으로 대상 주소를 변경)*
    
2. 이 명령어를 실행한다 (**`execute`**)
    
    

→ 위 과정을 반복하며 전체 코드를 읽고 수행한다 

## 운영체제의 역할

*우리가 직접 CPU를 통해 프로그램을 실행하게 하려면?*

> 실행 파일을 수동으로 메모리에 복사한 후, main 함수(진입점)에 해당하는 첫 번째 기계 명령어를 메모리에서 찾아 그 주소를 PC 레지스터에 적재하면 된다
> 

+) 추가 작업

- 한 번에 하나의 프로그램만 실행 가능 (멀티태스킹 불가)
- 프로그램에 사용할 하드웨어를 직접 특정 드라이버와 연결해야 함
- 기본적인 동작도 직접 구현해야 함 (e.g. print 함수)

→ 이 작업은 매우 복잡하고 번거로움. 이를 운영체제가 추가 작업과 더불어 한번에 처리해주는 것!

<aside>

정리하자면, **운영체제가 추상화하는 영역**은 다음과 같다. 

1. 메모리 적재 도구 (Loader)  “물리 메모리 용량에 무관하게, 자기자신이 CPU와 표준적인 메모리 크기를 독점한다고 간주”
2. 멀티태스킹  “시스템에서 자기자신 외 실행 중인 타 프로세스를 몰라도 됨”
3. 유틸리티 라이브러리
4. 상호작용 인터페이스
</aside>

### CPU는 한 번에 한 가지 일만 할 수 있다

하지만 여러 프로그램 사이를 빠르게 전환하며, 여러 가지 일을 “동시에” 실행되는 것처럼 보이게 할 수는 있다. 

*전환에 필요한 것은?*

프로그램의 실행 상태 = 상황 정보(context)

- Runtime stack frame

→ context를 저장하는 구조체를 **프로세스(process)**라고 부른다

### Multi-process Programming을 통해 실행 속도를 높이자!

func A, func B 의 관계가 서로 의존적이지 않다면 순차적으로 실행하는 것보다 동시에 실행하여 결과를 내는 것이 더 효율적일 것이다. 하지만, 이 두 개를 병렬로 수행하기 위해 프로세스를 무작정 나눠 버리면 실행 속도 측면에서는 효율이 높아질 지라도 그 외 문제점들이 여럿 따른다. 

- 프로세스 생성에 드는 오버헤드가 크다
- 프로세스 간 통신(IPC, inter-process communication) 시, 프로세스마다 가지는 자체적인 주소 공간에 의해 복잡도가 높다

→ CPU가 다음에 실행할 메모리 주소를 저장하는 **PC 레지스터**에 main 함수 (default) 외에도 다른 진입 함수를 지정한다면, 새로운 실행 흐름을 형성할 수 있다! 즉, 하나의 프로세스 안에 여러 실행 흐름이 존재할 수 있는 형태가 되고 이를 **스레드(thread)**라고 부른다

****프로세스 생성 시점에는 main 함수를 엔트리로 지정해야 하지만, 프로세스가 실행되는 중에는 여러 실행 흐름을 가질 수 있다***

**스레드 = 경량 프로세스 라고도 한다*

## 스레드 개념 이해하기

![alt text](image-3.png)
- Multi-Thread 의 장단점 (vs Multi-Process)
    - 프로세스와 달리 스레드는 하나의 프로세스 주소 공간을 공유하므로, 복잡한 IPC를 고려하지 않아도 된다
    - 하지만 여러 스레드가 공유 리소스에 접근할 때는 프로그래머가 직접 상호배제(mutual exclusion), 동기화(synchronization)를 이용해 명시적으로 문제를 해결해야 한다
- 스레드 처리 작업 유형
    
    **[Lifecycle 관점]** 
    
    - long task - Word, 디스크 기록
    - short task - 네트워크 요청, DB 쿼리
    
    **[작업 시 필요한 리소스 관점]**
    
    - CPU intensive task - 과학 연산, 행렬 연산 (외부 I/O에 의존X)
    - I/O intensive task - 연산 ↓ 디스크, 네트워크 등의 I/O ↑
- **요청당 스레드** : 요청이 들어올 때마다 매번 스레드가 생성되는 방식
    1. 작업 처리에 필요한 시간이 짧다
    2. 작업 수가 매우 많다
    
    → long task에는 유리하나, short task에서는 스레드의 생명주기를 매번 반복적으로 실행하며 전환해야 하는 오버헤드를 초래한다
    
- **스레드 풀(thread pool)**은 여러 개의 스레드를 미리 생성해두고, 필요에 따라 사용하고 반납하는 형식으로 리소스 효율적으로 재사용하는 방식이며, 위 문제를 해결해준다
    - 스레드 풀은 producer-consumer 패턴을 활용한 구현 전략 중 하나
    - pool = queue를 통해 관리되는 형태
    - pool 내 적절한 스레드 수는?
        - CPU intensive 할 때는 스레드 수 = CPU 코어 수일 때 충분히 CPU의 리소스를 활용할 수 있음
        - I/O intensive 할 때는 N x (1+WT÷CT) 의 공식이 존재  **하지만 실제로 정밀하게 측정하기는 어려운 값들이니, 실제 상황을 기반으로 결정할 것*
            - N = 코어 수
            - WT (Wait Time) = I/O 대기시간
            - CT (Computing Time) = CPU 연산에 필요한 시간
- thread context
    - 스레드마다 고유한 스택 프레임 영역을 가지며, 프로세스 주소 공간 내에는 스레드 수만큼 여러 스택 영역이 존재할 수 있다
    - 스레드 전용 리소스 외에는 전부 스레드 간 공유되는 리소스이다
        - 코드 영역 - read-only 이므로 thread-safe 이슈 없음
        - 데이터 영역 - 전역변수 단일 인스턴스에 접근하여 값을 변경하면 서로 영향을 줌
        - 힙 영역 - 포인터 정보만 알고 있다면 서로의 리소스에 접근 가능함
        - **스택 영역 - 스레드 전용 리소스로 스택 프레임이 할당되긴 하나, 엄밀히 따지면 격리된 전용 공간은 아니다**
            - 서로의 스레드에서 스택 프레임 포인터를 알 수 있다면, 접근 자체는 열려 있음!
            - 각 스레드에서의 동작이 서로에 영향을 주지 않는 격리 상태
            
            ```c
            __thread int a = 1;  // 스레드 전용 저장소
            ```
            

### Thread-Safe를 달성하려면?

> Multi-Thread 프로그래밍에서의 Thread-Safe는 필연적이다.
> 

***Thread-Safe란?***

어떤 코드가 주어졌을 때, 그 코드가 호출되는 스레드의 개수나 호출 순서에 무관하게 항상 올바른 결과임을 보장하는 것

[핵심] 코드가 **언제** 전용 리소스를 사용하고, **언제** 공유 리소스를 사용하는지만 잘 **구분**하면 된다!

- 전용 리소스만을 사용한다면 thread-safe를 달성할 수 있다 (`stateless function`)
- 공유 리소스를 사용한다면, 그에 맞는 안전장치(lock, semaphore, atomic 등)를 사용하여 리소스 순서를 유지할 때 thread-safe를 달성할 수 있다

****스레드 간 어떤 리소스라도 최대한 공유하지 않는 것이 원칙 (공유를 할 수밖에 없다면 명확하게 구분)*** 

[구분을 했다면 그 이후에는? - 각 증상에 맞는 약 처방!]

1. 스레드 전용 저장소(thread local storage) - 모든 스레드에서 사용할 수 있지만 각 스레드에서 자체 복사본을 가지는 형태로 선언 가능한지 확인
2. 읽기 전용(read-only) - 해당 전역 리소스를 읽기 전용으로 사용해도 되는지 확인
3. 원자성 연산(atomic operation) - 실행 중에 중단되지 않으므로 lock과 같은 별도 장치 불필요
4. 동기화 시 상호 배제(mutual exclusion in synchronization) - **개발자가 직접 공유 리소스의 순서를 제어해야 하는 상황 ⭐**
    1. mutex
    2. spin lock
    3. semaphore 

## 동기 방식으로 비동기 프로그래밍을 가능하게 하는 코루틴

- (kernel) 스레드의 이점인 상태 저장 → 일시 중지 및 재개를 지원
- 코루틴은 **사용자 상태 스레드**로, OS가 life cycle에 관여하는 것이 아닌 개발자가 직접 스케줄링 제어권을 가짐
    - e.g. Python의 `yield` 키워드 - “멈춰라”의 의미로 CPU 제어권 반환 가능
- 코루틴의 상태 정보 (함수의 스택 프레임에 저장 - 스레드와 달리 **힙 영역**에 저장)
    1. CPU 레지스터 정보
    2. 함수 실행 시 상태 정보 
    
    → 스레드 한 개당 코루틴은 N개 생길 수 있는 구조 (메모리 여유가 된다면 코루틴 개수는 무제한)
    

**fyi. 코루틴은 스레드보다 과거에 생긴 개념임*

### 콜백 함수

```c
void make_donut(func f)  // f가 콜백 함수에 해당
{
		...
		f();
		...
}
```

- 각자 정의한 localization function를 전달만 하면 되는 구조
    - f = 호출자(Caller)
    - make_donut = 피호출자(Callee)
- 정의한 쪽에서 함수의 정보들을 알고, 실제 호출은 다른 모듈이나 스레드에 맡기게 됨
    - 즉, 피호출자가 호출자를 호출하는 것이 **`콜백`**을 의미
    - 호출자는 **무엇**을 해야 하는지 알지만, **언제** 하는지 정확히 모름
    - 피호출자는 **언제** 하는지 알지만, **무엇**을 해야 하는지는 콜백 함수에 담긴 정보에 의존적으로 파악해야 함
- `비동기 콜백`  (asynchronous callback) **지연 콜백으로도 부름*
    - 이를 호출자와 피호출자가 각자의 스레드에서 병렬로 실행되게 한다면, 호출 스레드가 콜백 함수 실행에 의존하지 않는 형태가 됨
    - 호출할 downstream이 수십~수백 개에 이른다면 비동기 콜백 구조는 오히려 중첩이 과도해지면서 콜백 지옥에 빠질 수 있음
- Third party library의 구현, Event-driven programming에서의 ***handler***로서 적합
- fyi. `클로저(closure)` - 자신이 선언될 당시의 외부 스코프 변수를 함께 캡쳐한 함수 객체로, 이 특성을 이용하면 콜백 함수가 외부 상태를 기억한 채 전달되고 실행될 수 있다

|  | 동기 콜백 | 비동기 콜백 |
| --- | --- | --- |
| 블로킹 콜백 | O  *항상 | X |
| 논블로킹 콜백 | O | O |

### 동기 VS 비동기를 더 잘 이해해보자

[Keyword]

동기 - ***#종속적 #연관된 #기다림 #의존관계***

비동기 - ***#비종속적 #무관한 #기다릴 필요 없는 #동시 발생***

- 동기 호출
    - 동일한 스레드 내에서 실행되며, 순차적으로 실행이 완료될 때까지 기다려야 함
    - Blocking I/O
        1. 파일을 읽을 때 기본적으로 최하단 계층의 system call로 OS에 요청을 보냄
        2. 이때, 호출 스레드를 일시 중지시킴 (blocked)
        3. 커널이 디스크 내용을 읽어오면 다시 스레드 재개
        
        **CPU의 clock rate는 GHz 수준이므로, 디스크가 하나의 작업을 수행할 수 있는 ms 단위 시간이 CPU에 주어지면 대량의 기계 명령어 실행 작업을 수행할 수 있음 (⇒ 비동기가 훨씬 유리!)*
        
- 비동기 호출
    - 시간이 오래 걸리는 I/O 작업을 백그라운드 형태로 실행함
    - 흔히 2개의 스레드가 사용됨 (호출자, 피호출자)
    - 호출자는 즉시 반환되며, 결과에 대한 처리를 다음과 같이 나뉘어짐
        1. 호출자가 실행 결과를 전혀 신경 쓰지 않는 경우 - 콜백 함수 호출 방식
        2. 호출자가 실행 결과를 반드시 알아야 하는 경우 - 알림 작동 방식 (완료 시 signal 전송)

### Event-Driven Programming (Event-based Concurrency)

[구성 요소]

1. 이벤트 (event) - 대부분 입출력에 관계된 것
2. 이벤트 처리 함수 (event handler) - 이벤트를 처리하는 worker

- 이벤트를 반복적으로 수신하고 처리하는 Event Loop는 이벤트가 도착할 때까지 기다렸다가 대응하는 이벤트 핸들러를 호출하는 방식으로 진행된다
    - 하나의 함수로 여러 이벤트 가져오기 → **이벤트 소스와 입출력 다중화**
    - 핸들러(worker) 스레드와 Event Loop 스레드 구성 → 핸들러 특성에 따라 single or multi 스레드 택
        1. single - I/O가 전혀 없고 소요 시간이 매우 짧은 경우
        2. multi - CPU 시간을 많이 소모하는 작업일수록 multi thread가 유리
- 핸들러의 요청 처리 시 I/O가 포함된다면?
    1. I/O에 대응하는 논블로킹 인터페이스가 있는 경우 - Event Loop에서 직접 호출 가능
    2. I/O에 블로킹 인터페이스만 있는 경우 - 블로킹 I/O 호출이 포함된 작업은 worker 스레드로 전달해야 함
        
        *⚠️ 주의 - Event Loop 내에서는 절대 어떤 블로킹 인터페이스도 호출해서는 안 된다*

# Deep Dive
- **Event Loop가 실제로 어떻게 동작하는가**
- **Blocking I/O가 발생하면 어떻게 처리하는가**

[Optimizing web servers for high throughput and low latency](https://dropbox.tech/infrastructure/optimizing-web-servers-for-high-throughput-and-low-latency)

## Dropbox의 해결과제

Dropbox 는 서비스 특성상 ***정적 파일을 대량으로 서빙하는 서버*** 를 가진다. Dropbox의 Nginx 프록시 서버는 두 가지 성격의 요청을 동시에 처리해야 한다.

- **짧고 빠른 API 요청** (메타데이터 트랜잭션, 수만 TPS)
- **길고 무거운 파일 전송** (수십 Gbps)

*디스크가 HDD일 때, 캐시 미스가 잦을 때, 대용량 파일 서빙할 때* 병목이 발생하기 쉬운 구조이다. 

두 종류가 같은 서버에서 공존하며, 빠른 요청이 느린 요청 때문에 막혀서는 안 되고, 느린 요청이 빠른 요청 때문에 끊겨서도 안 된다. 이 문제를 해결하는 핵심이 바로 **Event Loop와 비동기 구조**이다

<aside>

**⚠️ Event Loop 내에서는 절대 어떤 Blocking 인터페이스도 호출해서는 안 된다.**

Blocking I/O가 포함된 작업은 Worker Thread로 전달해야 함

</aside>

→ 이 원칙이 Dropbox 에서 어떻게 실현되는가에 대한 내용을 다룸

### Nginx Event Loop

> *NGINX의 **이벤트 루프(Event Loop)**는 **수천 개의 동시 연결을 효율적으로 처리할 수 있게 해주는 핵심 아키텍처**입니다. 전통적인 웹 서버(Apache 등)가 요청마다 새로운 프로세스나 스레드를 생성하는 것과 달리, NGINX는 적은 수의 프로세스로 수만 개의 요청을 비동기적으로 관리합니다.*
> 

![alt text](image-4.png)

fyi. https://medium.com/@_sidharth_m_/how-nginx-handles-thousands-of-concurrent-requests-202ca1a1cc44

> *Nginx의 설계 철학: Worker Process로 코어를 나누고, 각 Worker 안에서 Event Loop로 수천 개의 연결을 처리. 공유 없이 병렬화*
> 

Master-Worker 구조를 통해 멀티 코어를 활용한다 (프로세스 레벨의 병렬화)

- **Master 프로세스의 역할**
    - 설정 파일 읽기
    - 포트 바인딩 (1024 이하 포트는 root 권한 필요)
    - Worker Process를 `fork()`로 생성
    - Worker Process 모니터링 및 재시작
- **Worker 프로세스의 역할**
    - 실제 클라이언트 요청 처리
    - 각자 독립적인 Event Loop 실행
    - 서로 메모리 공유 없음
    
    <aside>
    📚
    
    **스레드 간 공유 최소화 원칙 (프로세스 ver.)**
    
    스레드와 프로세스의 차이. 스레드는 같은 프로세스 내에서 메모리를 공유하지만, 프로세스는 각자 독립된 주소 공간을 가진다. Nginx는 공유 자원으로 인한 race condition 가능성을 아예 차단하기 위해 프로세스 레벨 격리를 택한다
    
    </aside>
    
- Worker 프로세스는 CPU에 고정되어 context switching이 발생하지 않는다. (CPU L1/L2 캐시가 항상 해당 Worker의 데이터로 유지되는 것!)
    
    ```bash
    worker_processes auto;        # CPU 코어 수만큼
    worker_cpu_affinity auto;     # 각 Worker를 특정 코어에 고정
    ```
    

**실제 흐름 (기대 동작):**

```mermaid
sequenceDiagram
    participant C as Client
    participant EL as Event Loop (단일 스레드)
    participant K as Kernel (epoll)

    C->>K: 요청 A 도착
    K->>EL: 이벤트 알림
    EL->>EL: Handler A 실행
    EL->>K: I/O 요청 (즉시 반환 ✅)
    
    C->>K: 요청 B 도착
    K->>EL: 이벤트 알림
    EL->>EL: Handler B 실행
    EL->>K: I/O 요청 (즉시 반환 ✅)

    K->>EL: A의 I/O 완료 신호
    EL->>EL: A 콜백 실행
    K->>EL: B의 I/O 완료 신호
    EL->>EL: B 콜백 실행
```

하나의 스레드가 이벤트를 받을 때마다 handler를 호출하고 즉시 다음 이벤트로 넘어가는 과정을 반복하며 non-blocking I/O로 수만 개의 연결을 동시에 처리할 수 있다

```
이벤트 루프: 이벤트 수신 → handler 호출 → 다음 이벤트
                              ↑________|
```

- 🤔 *왜 Nginx는 스레드를 쓰지 않을까?*
    
    ### Apache의 방식 — Thread/Process per Request
    
    전통적인 웹 서버인 Apache는 요청이 들어올 때마다 새로운 스레드(또는 프로세스)를 만들어 처리한다.
    
    ```bash
    요청 1 → Thread 1 (전담)
    요청 2 → Thread 2 (전담)
    요청 3 → Thread 3 (전담)
    ...
    요청 10,000 → Thread 10,000 (전담)
    ```
    
    직관적이고 구현이 단순하지만, 동시 접속자가 많아지면 치명적인 문제가 생긴다.
    
    - 스레드 1개당 메모리 약 1MB 소비 → 10,000개 = 10GB
    - 스레드 간 Context Switching 오버헤드 급증
    - 대부분의 스레드는 I/O를 기다리며 멍하니 CPU를 점유
    
    이걸 **C10K 문제**라고 한다. *“10,000개의 동시 연결을 어떻게 처리할 것인가”*
    
    <aside>
    📚
    
    스레드를 많이 만들수록 Context Switching 비용이 커진다. CPU는 스레드를 전환할 때마다 레지스터 상태를 저장/복원해야 한다.
    
    </aside>
    
    ### Nginx의 방식 — Event-Driven
    
    Nginx는 완전히 다른 접근을 택했다.
    
    - CPU 코어 수만큼만 Worker Process를 만든다 (보통 4~8개)
    - 각 Worker는 단일 스레드로 Event Loop를 돌린다
    - 하나의 Worker가 수천 개의 연결을 동시에 처리한다
    
    ```bash
    Worker 1 (단일 스레드)
        → 연결 A, B, C, D ... (수천 개) 동시 관리
    Worker 2 (단일 스레드)
        → 연결 E, F, G, H ... (수천 개) 동시 관리
    ```
    
    어떻게 단일 스레드가 수천 개의 연결을 처리할 수 있는가? → **`epoll`**
    
    **`epoll`은 이전 방식인 `select` / `poll`의 문제점을 해결하기 위해 등장한 방식*
    
    문제점 | 커널이 소켓 개수만큼 전부 순회하여 확인하는 O(N) 구조 → 연결이 많아질수록 선형적으로 느려짐
    
    해결방안 | 관심 있는 소켓들만 등록해두고, 준비된 소켓이 생기면 알리는 O(1) 구조 → 준비된 이벤트 수에만 비례
    
    ① epoll_create  → 커널에 epoll 인스턴스 생성 (감시 목록을 담는 자료구조)
    ② epoll_ctl     → 관심 있는 소켓 fd를 등록/수정/삭제
    ③ epoll_wait    → 준비된 이벤트가 생길 때까지 대기, 생기면 즉시 반환
    
    <aside>
    
    **리눅스 명령어**
    
    - `select` 싱글스레드로 여러 개의 파일을 작업하고자 할 때 사용할 수 있는 메커니즘
        - 입출력을 관리하고자 하는 파일의 그룹을  fd_set이라는 파일 비트 배열에 집어넣고, 이 배열의 값이 변했는지 확인하는 방식으로 동작
    - `poll` 함수를 통해 지정한 **소켓의 변화를 확인할 때** 쓰이는 함수
        - 소켓 set에 저장된 소켓의 변화가 생길 떄까지 기다리다가 소켓이 어떠한 동작을 하면 해당 소켓 외 나머지를 모두 제거하여 링크하는 방식
    - `epoll` 커널 레벨의 multiplexing을 지원 - 아주 적은 스레드가 아주 많은 I/O 대상(fd)을 번갈아서 처리할 수 있게 해주는 메커니즘
        - fd에 대한 루프를 돌 필요가 없고, 커널 공간이 fd를 관리하므로 빠른 이벤트 처리가 가능
    </aside>
    
    - epoll의 두 가지 모드
        
        
        | 모드 | 알림 시점 | 특징 |
        | --- | --- | --- |
        | Level-Triggered (LT) | 데이터가 **있는 동안** 계속 알림 | 구현 쉬움, 알림 중복 가능 |
        | Edge-Triggered (ET) | 상태가 **변할 때만** 한 번 알림 | 효율적, 데이터를 한 번에 전부 읽어야 함 |
        
        → Nginx는 ET(Edge-Triggered)를 사용하며, poll/select처럼 원래 LT만 지원하는 시스템에서도 Nginx는 이걸 ET처럼 변환해서 동작시킴
        
        ET 모드에서는 알림이 한 번만 오기 때문에 반드시 `EAGAIN`이 반환될 때까지 데이터를 전부 읽어야 한다. 구현이 복잡하지만 불필요한 syscall이 없어 성능이 좋다
        

### 비동기 콜백 in Nginx

<aside>
📚

**비동기 콜백 = 호출자와 피호출자가 각자의 스레드에서 병렬 실행, 완료 시 콜백 호출**

호출자는 **무엇**을 할지 알지만 **언제** 실행될지 모름

피호출자는 **언제** 실행할지 알지만 **무엇**을 할지는 콜백에 의존

</aside>

Nginx의 동작이 정확히 이 구조다.

- **Event Loop** = 피호출자. 이벤트가 도착하는 시점(언제)을 안다.
- **Handler 함수** = 호출자가 등록한 콜백. 무엇을 처리할지 담겨있다.
- **epoll** = 비동기 I/O 완료를 감지하는 커널 인터페이스. I/O가 끝나면 Event Loop에 알린다.

Thread Pool과 결합하면:

```
① Event Loop: "이 파일 읽기 작업 끝나면 이 콜백 호출해줘" → Thread Pool에 위임
② Thread Pool: 실제 I/O 수행 (blocking OK)
③ Thread Pool: 완료 → Event Loop에 신호
④ Event Loop: 콜백 실행
```

### Event Loop Stall

> *Nginx is an eventloop-based web server, which means it can only do one thing at a time. Even though it seems that it does all of these things simultaneously, like in time-division multiplexing, [all nginx does is just quickly switches between the events, handling one after another](https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/).*
> 

Nginx는 단일 스레드로 이벤트를 하나씩 순차 처리함. 이때 Handler 안에서 Disk I/O와 같은  **Blocking 호출**이 발생하면, 해당 스레드는 중단되며 Event Loop 전체가 멈추는 영향을 준다.

```mermaid
sequenceDiagram
    participant C as Client
    participant EL as Event Loop (단일 스레드)
    participant D as Disk

    C->>EL: 요청 A 도착
    EL->>EL: Handler A 실행
    EL->>D: read() 호출 ← Blocking! ❌
    
    Note over EL,D: ❌ 스레드 멈춤 (12~18ms)
    Note over EL: 이 동안 모든 요청 대기

    C-->>EL: 요청 B (대기 중...)
    C-->>EL: 요청 C (대기 중...)
    C-->>EL: 요청 D (대기 중...)

    D->>EL: 파일 읽기 완료
    EL->>EL: 이제서야 B, C, D 처리 시작
```

즉, 하나의 Blocking 호출이 서버 전체를 마비시키는 구조가 된다. Ch.2에서 배운 "Event Loop 내에서 Blocking 호출 금지" 원칙을 어겼을 때의 결과가 이것

### Dropbox가 실제로 측정한 Stall

 `funclatency` 툴로 측정한 `ngx_process_events_and_timers` 함수의 처리 시간

- 정상이라면 대부분의 이벤트가 1ms 이내로 처리되어야 함
- **이중 분포(bimodal distribution)** 가 보이면 Stall이 발생하는 것으로 진단
    - 수백만 번의 이벤트 중 일부가 수십 ms씩 멈추는 것이고, 그 순간 모든 요청이 대기하는 것을 의미

```
msecs  : count     분포
0→1    : 3799      ████████████████████████  ← 정상
2→15   : 0         (비어있음)
16→31  : 409       ████                      ← Stall!
32→63  : 313       ███
64→127 : 128       █
```

## 문제 원인 : 디스크 I/O

파일 읽기 같은 디스크 I/O는 기본적으로 Blocking이다. Nginx가 파일을 읽어야 할 때 Blocking `read()`를 호출하면 Event Loop 전체가 거기서 멈춘다. (앞서 언급한 주의사항)

```
nginx가 access.log 쓰기 → 18ms 지연
nginx가 파일 읽기       → 12ms 지연
```

18ms 동안 이 nginx worker는 아무 요청도 처리하지 못한 것이다.

### 타 이슈 사례 (Cloudflare)

> 
> 
> 
> https://blog.cloudflare.com/how-we-scaled-nginx-and-saved-the-world-54-years-every-day/
> 
> Cloudflare는 초당 1,000만 요청을 처리하는 환경에서 `read()` 이전에 `open()` syscall 하나가 Event Loop를 막는다는 것을 발견함
> 
> - `aio threads`를 설정해 **`read()`를 Thread Pool로 offload**했는데 효과는 없었음
>     - Nginx 공식 벤치마크가 **4MB 파일 + HDD** 환경이라 `read()` 자체가 오래 걸려서 효과가 컸던 거고, Cloudflare는 **60KB 이하 파일 + SSD** 라 `read()` 시간이 짧아서 상대적으로 `open()` 비중이 더 컸던 것 (같은 설정이라도 환경에 따라 효과는 달라질 수 있다)
> - `open()`이 하는 일 **(→ 아래 과정이 모두 Blocking으로 진행된 것)**
>     1. 파일 경로로 inode 찾기
>     2. 디렉토리 탐색 (/cache/prefix/dir/EF/BE/CAFEBEEF)
>         
>         *내부적으로 경로 탐색을 위해 6번의 inode 조회를 함
>         
>         ```bash
>         /cache
>         /cache/prefix
>         /cache/prefix/dir
>         /cache/prefix/dir/EF
>         /cache/prefix/dir/EF/BE
>         /cache/prefix/dir/EF/BE/CAFEBEEF
>         ```
>         
>     3. 권한 확인
> 
> **`open()`도 Thread Pool로 옮기는 패치**를 적용하여 결과적으로 ***p99 TTFB가 6배 개선됨***
> 

## 해결 방안 (Blocking I/O 방지 수단)

### 1. **`aio threads`** Thread Pool Offload

<aside>
📚

Blocking I/O가 포함된 작업은 Worker Thread로 전달하기

Event Loop 스레드와 Worker 스레드 분리하여 구성

</aside>

Dropbox의 실제 구현이 이 방식을 따른다.  http://nginx.org/en/docs/ngx_core_module.html#thread_pool

```
**aio threads;**
aio_write on;
```

**[설정의 의미]**

```mermaid
sequenceDiagram
    participant C as Client
    participant EL as Event Loop (단일 스레드)
    participant TP as Thread Pool (Worker Threads)
    participant D as Disk

    C->>EL: 요청 A 도착
    EL->>TP: "파일 읽기 끝나면 콜백 호출해줘" (위임)
    EL-->>EL: 즉시 다음 이벤트로

    C->>EL: 요청 B 도착
    EL->>EL: Handler B 실행 (정상 처리)

    TP->>D: read() 호출 (Blocking이어도 OK, 여기선 허용)
    D->>TP: 파일 읽기 완료
    TP->>EL: 완료 신호
    EL->>EL: A 콜백 실행
```

Blocking 작업이 별도의 Worker Thread 풀로 들어갔으니 Event Loop는 멈추지 않고 그 사이에도 다른 요청을 계속 처리할 수 있다.

- **효과:** Nginx 공식 벤치마크 기준 처리량 **최대 9배 향상**, D-state(디스크 대기) 프로세스 소멸

### 2. `sendfile on` 으로 파일 전송 시 커널이 직접 처리

User space ↔ Kernel space 복사 없이 네트워크 카드로 바로 전송. 소켓 → 파일 전송에서 Blocking을 근본적으로 우회하는 방식

### 3. **`aio on` + `directio`** Linux 커널 AIO 인터페이스 사용

⚠️ 제약사항 - 파일을 반드시 4KB 정렬된 크기로 읽어야 하고, OS 페이지 캐시를 무조건 bypass(= 항상 디스크에서 직접 읽음). 그래서 소규모 파일엔 오히려 독이고, 8MB 이상 대용량 파일 전송에만 적합

**[일반적인 권장 조합]**

```bash
sendfile on;          # 작은 파일
aio threads;          # 큰 파일 (Thread Pool)  
directio 8m;          # 8MB 이상은 directio
```

**[Nginx Thread Pool Default 설정 값]**

```bash
# 기본 Thread Pool (빌드 시 --with-threads 필요)
thread_pool default threads=32 max_queue=65536;

# 디스크별로 분리 (디스크 I/O 병렬화)
thread_pool disk1 threads=16 max_queue=65536;
thread_pool disk2 threads=16 max_queue=65536;

http {
    location /files/ {
        aio threads=disk1;
        aio_write on;
    }
}
```

| 설정 | Default |
| --- | --- |
| `aio` | **off** |
| `sendfile` | **off** |
| `aio threads` (Thread Pool) | **비활성화** — 빌드 시 `--with-threads` 플래그 필요
→ 빌드 자체를 다시 해야 쓸 수 있어서 진입장벽이 있음 |
| Thread Pool 기본 스펙 | 스레드 32개, 큐 최대 65,536개 |

*Socket I/O와 달리 Disk I/O는 `epoll`을 통해 해결이 불가능하다

### +) 로그 쓰기도 같은 문제

로그 파일 쓰기도 Blocking임 → Dropbox는 **로그를 `syslog`로 전달**함으로써 이 문제를 해결

```
access_log syslog:server=unix:/dev/log;
```

syslog는 **비동기로 동작**하기 때문에 Event Loop에서 직접 호출해도 멈추지 않는다. 혹은 메모리 버퍼에 모았다가 일정량이 쌓이면 쓰는 방식으로도 Blocking 빈도를 줄일 수 있다.

```
access_log /var/log/nginx/access.log combined buffer=64k;
```
