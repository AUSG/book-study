# 웹 로봇
웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램

### 크롤러
웹 링크를 통해 재귀적으로 따라가는 로봇

크롤러들의 동작 방법
1. 루트 집합에서 시작
2. 링크 추출과 상대링크 정상화
3. 순환 피하기루프와 중복 해결
4. 방문한 곳 관리
5. 별칭으로 인한 로봇 순환 해결
6. URL 정규화
7. 파일 시스템 링크 순환 해결


### 로봇의 HTTP

1. 요청헤더를 최대한 채워주자
2. 가상 호스팅이 널려있기 때문에 Host 헤더를 꼭 보내라
3. 변경이 있을 때만 콘텐츠를 받아오기 위해 조건부 HTTP 요청을 사용해라
4. 응답 다루기
5. User-Agent 타겟팅
    1. 웹 관리자들은 많은 로봇이 방문할 것임을 인지하고, 요청을 예상해야한다.
    2. 로봇을 차단하거나, 로봇이 SEO (검색엔진 최적화) 잘 할 수 있도록 하거나..
6. 로봇 제작자들은 특정 페이지에 엄청난 요청을 보낼 수 있기 때문에, 이러한 순환에 빠지지 않도록 해야한다.

### robots.txt

1. robots.txt 가 있는 경우, 이를 통해 방문 가능한 페이지만 접근하도록 로봇을 만들어야 한다.

로봇의 처리 규칙
1. robots.txt를 응답받으면 그거 따라가기
2. 404 에러코드를 받으면, 차단 규칙이 존재하지 않는다고 가정하고 제약없이 접근 가능
3. 접근 제한으로 응답하면 사이트 접근 완전히 제한되어있다고 가정
4. 요청 시도가 일시직으로 실패하면 리소스 검색을 뒤로 미루기
5. 서버 응답이 리다이렉션을 의미한다면 리소스가 발견될 때까지 리다이렉트 따라가기